import requests
import json
import ollama
import openai
from openai import OpenAI
from app.utils.entity import extract_entity_via_llm
import re
from py2neo import Graph

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import re
def api_call(query):
    prompt = f"""
é˜…è¯»ä¸‹åˆ—æç¤ºï¼Œå›ç­”é—®é¢˜ï¼ˆé—®é¢˜åœ¨è¾“å…¥çš„æœ€åï¼‰:
å½“ä½ è¯•å›¾è¯†åˆ«ç”¨æˆ·é—®é¢˜ä¸­çš„æŸ¥è¯¢æ„å›¾æ—¶ï¼Œä½ éœ€è¦ä»”ç»†åˆ†æé—®é¢˜ï¼Œå¹¶åœ¨é¢„å®šä¹‰çš„è¯¾ç¨‹æŸ¥è¯¢ç±»åˆ«ä¸­é€ä¸€è¿›è¡Œåˆ¤æ–­ã€‚å¯¹äºæ¯ä¸€ä¸ªç±»åˆ«ï¼Œæ€è€ƒç”¨æˆ·çš„é—®é¢˜æ˜¯å¦å«æœ‰ä¸è¯¥ç±»åˆ«å¯¹åº”çš„æ„å›¾ã€‚å¦‚æœç¬¦åˆï¼Œå°±å°†è¯¥ç±»åˆ«åŠ å…¥è¾“å‡ºåˆ—è¡¨ä¸­ã€‚

**æŸ¥è¯¢ç±»åˆ«**
- "æŸ¥è¯¢è¯¾ç¨‹æè¿°"
- "æŸ¥è¯¢è¯¾ç¨‹å­¦åˆ†"
- "æŸ¥è¯¢è¯¾ç¨‹æ€»å­¦æ—¶"
- "æŸ¥è¯¢è¯¾ç¨‹ç†è®ºå­¦æ—¶"
- "æŸ¥è¯¢è¯¾ç¨‹å®éªŒå­¦æ—¶"
- "æŸ¥è¯¢è¯¾ç¨‹è€ƒæ ¸æ–¹å¼"
- "æŸ¥è¯¢è¯¾ç¨‹æ•™æ"
- "æŸ¥è¯¢è¯¾ç¨‹å‚è€ƒä¹¦ç›®"
- "æŸ¥è¯¢è¯¾ç¨‹é€‚ç”¨ä¸“ä¸š"
- "æŸ¥è¯¢è¯¾ç¨‹ç›¸å…³è¯¾ç¨‹"
- "æŸ¥è¯¢è¯¾ç¨‹æ•™å­¦ç›®æ ‡"
- "æŸ¥è¯¢è¯¾ç¨‹å…ˆä¿®è¯¾ç¨‹"
- "æŸ¥è¯¢è¯¾ç¨‹æ‰€æœ‰ç« èŠ‚"
- "æŸ¥è¯¢è¯¾ç¨‹æŸä¸€ç« èŠ‚"
- "æŸ¥è¯¢è¯¾ç¨‹æŸä¸€çŸ¥è¯†ç‚¹"
**ç¤ºä¾‹**
è¾“å…¥ï¼š"è¿™é—¨è¯¾è®²å•¥çš„ï¼Ÿ"
è¾“å‡ºï¼š["æŸ¥è¯¢è¯¾ç¨‹æè¿°"] 

è¾“å…¥ï¼š"æ•°æ®ç»“æ„è€ƒä»€ä¹ˆï¼Ÿ"
è¾“å‡ºï¼š["æŸ¥è¯¢è¯¾ç¨‹è€ƒæ ¸æ–¹å¼", "æŸ¥è¯¢è¯¾ç¨‹æè¿°"]  

è¾“å…¥ï¼š"äººå·¥æ™ºèƒ½è¿™é—¨è¯¾æ¨èå“ªäº›æ•™æï¼Ÿ"
è¾“å‡ºï¼š["æŸ¥è¯¢è¯¾ç¨‹æ•™æ"]  

è¾“å…¥ï¼š"æˆ‘è¿™ä¸ªä¸“ä¸šå¯ä»¥é€‰è¿™é—¨è¯¾å—ï¼Ÿ"
è¾“å‡ºï¼š["æŸ¥è¯¢è¯¾ç¨‹é€‚ç”¨ä¸“ä¸š"]  

è¾“å…¥ï¼š"æ•°æ®åº“è¯¾ç¨‹ä¸€å…±å‡ å­¦æ—¶ï¼Ÿ"
è¾“å‡ºï¼š["æŸ¥è¯¢è¯¾ç¨‹æ€»å­¦æ—¶"] 

**ä»»åŠ¡è¦æ±‚**
- è¾“å‡ºå¿…é¡»ä»…é™ä¸Šè¿°12ä¸ªç±»åˆ«ä¸­é€‰ï¼Œä¸å¾—åˆ›é€ æ–°åè¯ã€‚
- è¾“å‡ºæ„å›¾æ•°é‡ä¸èƒ½è¶…è¿‡5ä¸ªã€‚
- å¦‚æœé—®é¢˜æ¶‰åŠè¯¾ç¨‹ï¼Œä¸€èˆ¬éƒ½ä¼šæœ‰â€œæŸ¥è¯¢è¯¾ç¨‹æè¿°â€çš„éœ€æ±‚ï¼Œè¯·ä¼˜å…ˆè€ƒè™‘ã€‚
- ä»…è¾“å‡ºæŸ¥è¯¢åˆ°çš„æ„å›¾ç»“æœåˆ—è¡¨ï¼

ç°åœ¨è¯·è¯†åˆ«ä¸‹é¢è¿™ä¸ªé—®é¢˜çš„æ„å›¾ï¼š
é—®é¢˜è¾“å…¥ï¼š"{query}"
è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
["æŸ¥è¯¢è¯¾ç¨‹æè¿°", "æŸ¥è¯¢è¯¾ç¨‹æ•™æ"] 

"""
    '''
    **ç¤ºä¾‹**
    è¾“å…¥ï¼š"è¿™é—¨è¯¾è®²å•¥çš„ï¼Ÿ"
    è¾“å‡ºï¼š["æŸ¥è¯¢è¯¾ç¨‹æè¿°"]  # æ˜æ˜¾æƒ³äº†è§£è¯¾ç¨‹å†…å®¹

    è¾“å…¥ï¼š"æ•°æ®ç»“æ„è€ƒä»€ä¹ˆï¼Ÿ"
    è¾“å‡ºï¼š["æŸ¥è¯¢è¯¾ç¨‹è€ƒæ ¸æ–¹å¼", "æŸ¥è¯¢è¯¾ç¨‹æè¿°"]  # æƒ³çŸ¥é“æ€ä¹ˆè€ƒï¼Œè¿˜å¯èƒ½æƒ³äº†è§£è¯¾çš„å†…å®¹

    è¾“å…¥ï¼š"äººå·¥æ™ºèƒ½è¿™é—¨è¯¾æ¨èå“ªäº›æ•™æï¼Ÿ"
    è¾“å‡ºï¼š["æŸ¥è¯¢è¯¾ç¨‹æ•™æ"]  # è¯¢é—®æ•™æ

    è¾“å…¥ï¼š"æˆ‘è¿™ä¸ªä¸“ä¸šå¯ä»¥é€‰è¿™é—¨è¯¾å—ï¼Ÿ"
    è¾“å‡ºï¼š["æŸ¥è¯¢è¯¾ç¨‹é€‚ç”¨ä¸“ä¸š"]  # æƒ³çŸ¥é“è¯¾ç¨‹é€‚ç”¨äºå“ªäº›ä¸“ä¸š

    è¾“å…¥ï¼š"æ•°æ®åº“è¯¾ç¨‹ä¸€å…±å‡ å­¦æ—¶ï¼Ÿ"
    è¾“å‡ºï¼š["æŸ¥è¯¢è¯¾ç¨‹æ€»å­¦æ—¶"]  # è¯¢é—®è¯¾ç¨‹æ—¶é•¿

    **ä»»åŠ¡è¦æ±‚**
    - è¾“å‡ºå¿…é¡»ä»…é™ä¸Šè¿°12ä¸ªç±»åˆ«ä¸­é€‰ï¼Œä¸å¾—åˆ›é€ æ–°åè¯ã€‚
    - è¾“å‡ºæ„å›¾æ•°é‡ä¸èƒ½è¶…è¿‡5ä¸ªã€‚
    - è¾“å‡ºåç´§è·Ÿæ³¨é‡Šï¼Œç”¨"#"ç®€è¦è¯´æ˜åˆ¤æ–­ä¾æ®ã€‚
    - å¦‚æœé—®é¢˜æ¶‰åŠè¯¾ç¨‹ï¼Œä¸€èˆ¬éƒ½ä¼šæœ‰â€œæŸ¥è¯¢è¯¾ç¨‹æè¿°â€çš„éœ€æ±‚ï¼Œè¯·ä¼˜å…ˆè€ƒè™‘ã€‚

    ç°åœ¨è¯·è¯†åˆ«ä¸‹é¢è¿™ä¸ªé—®é¢˜çš„æ„å›¾ï¼š
    é—®é¢˜è¾“å…¥ï¼š"{query}"
    è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
    ["æŸ¥è¯¢è¯¾ç¨‹æè¿°", "æŸ¥è¯¢è¯¾ç¨‹æ•™æ"]  # è¯¥é—®é¢˜æƒ³äº†è§£è¯¾ç¨‹çš„å†…å®¹å’Œä½¿ç”¨æ•™æ
    '''
    # choice = 'qwen2:latest'
    # try:
    #     rec_result = ollama.generate(model=choice, prompt=prompt)['response']
    #     print(f'æ„å›¾è¯†åˆ«ç»“æœ:{rec_result}')
    #     return rec_result

    # except Exception as e:
    #     # æ•è·ä»»ä½•å¼‚å¸¸å¹¶è®°å½•æ—¥å¿—
    #     print(f"An error occurred: {e}")
    # æ„å»ºåˆå§‹æ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…æ‹¬ç³»ç»Ÿè§’è‰²å’Œç”¨æˆ·è§’è‰²
    message = [{"role":"system","content":"ä½ æ˜¯ä¸€ä¸ªæ„å›¾è¯†åˆ«ä¸“å®¶ã€‚"}]
    # å°†ç”¨æˆ·è¾“å…¥çš„promptæ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­
    message.append({"role":"user","content":prompt})
    
    # è®¾ç½®APIè°ƒç”¨çš„URL
    url="http://202.127.200.34:30025/v1/chat/completions"
    
    # è®¾ç½®HTTPè¯·æ±‚å¤´ï¼ŒæŒ‡å®šå†…å®¹ç±»å‹ä¸ºJSON
    headers = {'Content-Type': 'application/json'}
    
    # æ„å»ºè¯·æ±‚æ•°æ®ï¼ŒåŒ…æ‹¬æ¨¡å‹åç§°ã€æ¶ˆæ¯åˆ—è¡¨ã€æ¸©åº¦å’Œæœ€å¤§ä»¤ç‰Œæ•°
    deta={
        "model": "qwen2-7B",
        "messages": message,
        "temperature": 0.5,
        "max_tokens": 2048
    }
    
    # å‘é€POSTè¯·æ±‚åˆ°API
    response = requests.post(url, json=deta, headers=headers)
    
    # è·å–å“åº”æ–‡æœ¬
    b=response.text
    
    # å°†å“åº”æ–‡æœ¬è§£æä¸ºJSONå¯¹è±¡
    d=json.loads(b)
    
    # è¿”å›è§£æåçš„JSONå¯¹è±¡ä¸­ç¬¬ä¸€ä¸ªé€‰æ‹©é¡¹çš„æ¶ˆæ¯å†…å®¹
    return d['choices'][0]['message']['content']



def ollama_call(query):
    choice="deepseek-r1:70b"
    try:
        rec_result = ollama.generate(model=choice, prompt=query)['response']
        print(f'æ„å›¾è¯†åˆ«ç»“æœ:{rec_result}')
        # return rec_result

    except Exception as e:
        # æ•è·ä»»ä½•å¼‚å¸¸å¹¶è®°å½•æ—¥å¿—
        print(f"An error occurred: {e}")
    #  rec_result = ollama.generate(model=choice, prompt=query)['response']
    #     print(f'æ„å›¾è¯†åˆ«ç»“æœ:{rec_result}')
def openai_call(query):
    # è®¾ç½®è‡ªå®šä¹‰ API åŸºç¡€ URL å’Œ API å¯†é’¥
    # è°ƒç”¨ OpenAI API
    try:
        client = OpenAI(
        api_key="sk-ySV9lh42U0XP275mPdXOEYy6ygnva683aBIHvG1Ol7sEk6Nt", 
        base_url="https://api.claudeshop.top/v1",    )

        chat_completion = client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": query,
                }
            ],
            model="gpt-4",
        )
        message=chat_completion.choices[0].message.content
        print(message)
    except Exception as e:
        print(f"An error occurred: {e}")

# æ˜ å°„æ„å›¾åˆ°è¯¾ç¨‹çŸ¥è¯†å›¾è°±ä¸­çš„å±æ€§æˆ–å…³ç³»å­—æ®µ
INTENT_TO_KG_FIELD = {
    "æŸ¥è¯¢è¯¾ç¨‹æè¿°": {"type": "node", "property": "æè¿°"},
    "æŸ¥è¯¢è¯¾ç¨‹å­¦åˆ†": {"type": "node", "property": "å­¦åˆ†"},
    "æŸ¥è¯¢è¯¾ç¨‹æ€»å­¦æ—¶": {"type": "node", "property": "æ€»å­¦æ—¶"},
    "æŸ¥è¯¢è¯¾ç¨‹ç†è®ºå­¦æ—¶": {"type": "node", "property": "ç†è®ºå­¦æ—¶"},
    "æŸ¥è¯¢è¯¾ç¨‹å®éªŒå­¦æ—¶": {"type": "node", "property": "å®éªŒå­¦æ—¶"},
    "æŸ¥è¯¢è¯¾ç¨‹è€ƒæ ¸æ–¹å¼": {"type": "node", "property": "è€ƒæ ¸æ–¹å¼"},
    "æŸ¥è¯¢è¯¾ç¨‹æ•™æ": {"type": "relation", "rel": "ä½¿ç”¨æ•™æ", "target": "æ•™æ"},
    "æŸ¥è¯¢è¯¾ç¨‹å‚è€ƒä¹¦ç›®": {"type": "relation", "rel": "å‚è€ƒèµ„æ–™", "target": "å‚è€ƒä¹¦ç›®"},
    "æŸ¥è¯¢è¯¾ç¨‹é€‚ç”¨ä¸“ä¸š": {"type": "relation", "rel": "é€‚ç”¨ä¸“ä¸š", "target": "ä¸“ä¸š"},
    "æŸ¥è¯¢è¯¾ç¨‹ç›¸å…³è¯¾ç¨‹": {"type": "relation", "rel": "ç›¸å…³è¯¾ç¨‹", "target": "è¯¾ç¨‹"},
    "æŸ¥è¯¢è¯¾ç¨‹æ•™å­¦ç›®æ ‡": {"type": "node", "property": "æ•™å­¦ç›®æ ‡"},  # å¯é€‰å­—æ®µï¼Œè‹¥æœ‰æ·»åŠ 
    "æŸ¥è¯¢è¯¾ç¨‹å…ˆä¿®è¯¾ç¨‹": {"type": "relation", "rel": "å…ˆä¿®è¯¾ç¨‹", "target": "è¯¾ç¨‹"},  # éœ€æå‰å»ºå›¾
    "æŸ¥è¯¢è¯¾ç¨‹æ‰€æœ‰ç« èŠ‚": {"type": "multi-hop", "subtype": "all_units"},
    "æŸ¥è¯¢è¯¾ç¨‹ç« èŠ‚": {"type": "multi-hop", "subtype": "all_units"},
    "æŸ¥è¯¢è¯¾ç¨‹æŸä¸€ç« èŠ‚": {"type": "multi-hop", "subtype": "specific_unit"},
    "æŸ¥è¯¢è¯¾ç¨‹æŸä¸€çŸ¥è¯†ç‚¹": {"type": "multi-hop", "subtype": "specific_kp"},
}

# è¿æ¥ Neo4j
graph = Graph("http://localhost:7474", user="neo4j", password="cz666888*", name="neo4j")

def query_specific_unit_kg(course_name, query_text):
    print("è¾“å…¥",course_name,query_text)

    """
    åŸºäºè¯¾ç¨‹å+ç« èŠ‚ç¼–å·ï¼ˆå¦‚â€œç¬¬ä¸€ç« â€ï¼‰ç²¾å‡†æŸ¥è¯¢ç« èŠ‚çš„çŸ¥è¯†ç‚¹å†…å®¹
    """
    # åŒ¹é…â€œç¬¬Xç« â€ï¼ˆæ”¯æŒæ•°å­—æˆ–ä¸­æ–‡ï¼‰
    match = re.search(r"ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡0-9]+)ç« ", query_text)
    if not match:
        return "æœªè¯†åˆ«å‡ºç« èŠ‚å"

    unit_part = match.group(1)

    # ä¸­æ–‡æ•°å­—ç®€æ˜“è½¬æ¢
    zh_to_num = {"ä¸€": "1", "äºŒ": "2", "ä¸‰": "3", "å››": "4", "äº”": "5",
                 "å…­": "6", "ä¸ƒ": "7", "å…«": "8", "ä¹": "9", "å": "10"}
    if unit_part.isdigit():
        unit_id = unit_part
    else:
        unit_id = "".join([zh_to_num.get(c, c) for c in unit_part])
    print("å•ç« æŸ¥è¯¢",course_name,unit_id)
    # æŸ¥è¯¢å›¾è°±
    cypher = """
    MATCH (c:è¯¾ç¨‹ {åç§°: $course_name})-[:åŒ…å«çŸ¥è¯†ç‚¹]->(kp:çŸ¥è¯†ç‚¹)
          -[:å±äºç« èŠ‚]->(u:ç« èŠ‚ {ç¼–å·: $unit_id}),
          (kp)-[:è§£é‡Šä¸º]->(concept:çŸ¥è¯†ç‚¹å®šä¹‰)
    RETURN u.åç§° AS ç« èŠ‚, kp.åç§° AS çŸ¥è¯†ç‚¹, concept.å†…å®¹ AS å†…å®¹ä»‹ç»
    ORDER BY kp.åç§°
    """
    result = graph.run(cypher, course_name=course_name, unit_id=unit_id).data()

    if not result:
        return "æœªæ‰¾åˆ°è¯¥ç« èŠ‚å†…å®¹"

    return "\n".join([f"[{r['ç« èŠ‚']}] {r['çŸ¥è¯†ç‚¹']}: {r['å†…å®¹ä»‹ç»']}" for r in result])

# æŸ¥è¯¢æ–¹å¼ä¸€ï¼šæ¨¡ç³Šè¯¾ç¨‹åï¼ˆæ­£åˆ™ï¼‰
# æŸ¥è¯¢æ–¹å¼ä¸€ï¼šæ¨¡ç³Šè¯¾ç¨‹åï¼ˆæ­£åˆ™ï¼‰
def fuzzy_query_by_course_name(course_name_pattern, query_type=0):
    print("ğŸŸ¡ åŸå§‹è¯¾ç¨‹åç§°æ¨¡å¼è¾“å…¥:", course_name_pattern)

    # æ­£åˆ™åŒ…è£…
    if not course_name_pattern.startswith(".*"):
        course_name_pattern = f".*{re.escape(course_name_pattern)}.*"

    print("ğŸŸ¢ æ­£åˆ™æ¨¡å¼ after escape:", course_name_pattern)

    query = """
    MATCH (c:è¯¾ç¨‹)-[:åŒ…å«çŸ¥è¯†ç‚¹]->(kp:çŸ¥è¯†ç‚¹)-[:å±äºç« èŠ‚]->(u:ç« èŠ‚),
          (kp)-[:è§£é‡Šä¸º]->(concept:çŸ¥è¯†ç‚¹å®šä¹‰)
    WHERE c.åç§° =~ $pattern
    RETURN c.åç§° AS è¯¾ç¨‹, u.åç§° AS ç« èŠ‚, u.ç¼–å· AS ç« èŠ‚ç¼–å·,
           kp.åç§° AS çŸ¥è¯†ç‚¹, concept.å†…å®¹ AS å†…å®¹ä»‹ç»
    ORDER BY u.ç¼–å·
    """

    result = graph.run(query, pattern=course_name_pattern).data()

    print(f"ğŸ”µ æŸ¥è¯¢ç»“æœæ•°é‡: {len(result)}")

    if query_type == 1:
        # è¿”å›çŸ¥è¯†ç‚¹åˆ—è¡¨
        knowledge_points = sorted(set(r["çŸ¥è¯†ç‚¹"] for r in result if "çŸ¥è¯†ç‚¹" in r))
        print(f"ğŸ§  è¿”å›çŸ¥è¯†ç‚¹æ€»æ•°: {len(knowledge_points)}")
        return knowledge_points

    elif query_type == 2:
        # è¿”å›ç« èŠ‚ä¿¡æ¯
        chapters = sorted(set((r["ç« èŠ‚"], r["ç« èŠ‚ç¼–å·"]) for r in result if "ç« èŠ‚" in r))
        print(f"ğŸ“š è¿”å›ç« èŠ‚æ€»æ•°: {len(chapters)}")
        return [{"è¯¾ç¨‹": course_name_pattern, "ç« èŠ‚": ch[0], "ç« èŠ‚ç¼–å·": ch[1]} for ch in chapters]

    else:
        # è¿”å›å®Œæ•´ä¿¡æ¯
        return result




def query_specific_kp_kg(course_name, query_text):
    print("ğŸŸ¢ åŸå§‹è¾“å…¥ï¼š", query_text)
    print("ğŸ“˜ è¯¾ç¨‹åï¼š", course_name)

    # å»æ‰è¯¾ç¨‹åï¼Œä¿ç•™å…³é”®è¯éƒ¨åˆ†
    cleaned_text = query_text.replace(course_name, "")
    print("ğŸ§¹ æ¸…ç†åçš„æ–‡æœ¬ï¼š", cleaned_text)

    # åŒ¹é…å…³é”®è¯
    match = re.search(r"(?:ä¸­çš„|å…³äº)?([\u4e00-\u9fa5a-zA-Z0-9]{2,30})", cleaned_text)
    keyword = match.group(1).strip() if match else None

    if not keyword or keyword in course_name:
        return "æœªè¯†åˆ«å‡ºæœ‰æ•ˆçš„çŸ¥è¯†ç‚¹å…³é”®è¯"

    # å»é™¤æ— æ„ä¹‰åç¼€
    keyword = re.sub(r"(æ˜¯ä»€ä¹ˆ|æœ‰å“ªäº›|çš„å†…å®¹|å†…å®¹|ä»‹ç»)?$", "", keyword)
    print("ğŸ” æå–çš„çŸ¥è¯†ç‚¹å…³é”®è¯:", keyword)

    # æ­£åˆ™åŒ¹é…æ¨¡å¼ï¼ˆæ¨¡ç³Šï¼‰
    keyword_pattern = f".*{re.escape(keyword)}.*"

    # å›¾è°±æŸ¥è¯¢ï¼šæ­£åˆ™åŒ¹é…çŸ¥è¯†ç‚¹åç§°
    cypher = """
    MATCH (c:è¯¾ç¨‹)-[:åŒ…å«çŸ¥è¯†ç‚¹]->(kp:çŸ¥è¯†ç‚¹)-[:è§£é‡Šä¸º]->(concept:çŸ¥è¯†ç‚¹å®šä¹‰)
    WHERE c.åç§° CONTAINS $course_name AND kp.åç§° =~ $pattern
    RETURN kp.åç§° AS çŸ¥è¯†ç‚¹, concept.å†…å®¹ AS å†…å®¹ä»‹ç»
    """
    result = graph.run(cypher, course_name=course_name, pattern=keyword_pattern).data()
    print(f"ğŸ” åŒ¹é…ç»“æœæ•°é‡: {len(result)}")

    if not result:
        matches = find_kp_by_embedding(query_text, top_k=3)

        if not matches:
            return "æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†ç‚¹ï¼ˆå‘é‡åŒ¹é…å¤±è´¥ï¼‰"
        name, text, score = matches[0]
        return f"{name}:\n{text}\nï¼ˆç›¸ä¼¼åº¦: {score:.3f}ï¼‰"

    return "\n".join([f"{r['çŸ¥è¯†ç‚¹']}: {r['å†…å®¹ä»‹ç»']}" for r in result])


def find_kp_by_embedding(query_text, top_k=1):
    from transformers import AutoTokenizer, AutoModel
    import torch

    model_path = "/home/ubuntu/.cache/modelscope/hub/models/BAAI/bge-m3"
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModel.from_pretrained(model_path).eval()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    def encode(text):
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(device)
        with torch.no_grad():
            outputs = model(**inputs)
            embeddings = outputs.last_hidden_state[:, 0]
            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
        return embeddings.cpu().numpy()

    with open("/media/zhjk/rmx/medical/tcm_graph/KGLLM/KnowledgeGraph-based-on-Raw-text-A27-main/server/all_course_embeddings_bge_m3.json", "r", encoding="utf-8") as f:
        all_data = json.load(f)

    query_vec = encode(query_text)

    sims = []
    for item in all_data:
        vec = np.array(item["embedding"])
        sim = cosine_similarity(query_vec, vec.reshape(1, -1))[0][0]
        sims.append((sim, item))

    sims.sort(reverse=True)
    top_results = sims[:top_k]

    return [(r[1]["name"], r[1]["text"], r[0]) for r in top_results if r[0] > 0.4]

def multi_hop_query_kg(course_name, intent, query_text=None):
    config = INTENT_TO_KG_FIELD[intent]
    subtype = config.get("subtype")

    if subtype == "all_units":
        res=fuzzy_query_by_course_name(course_name,query_type=2)
    
        return res

    elif subtype == "specific_unit":
        # æå–â€œç¬¬å‡ ç« â€æˆ–ç« èŠ‚å
        res=query_specific_unit_kg(course_name,query_text)
        return res

    elif subtype == "specific_kp":
        res=query_specific_kp_kg(course_name,query_text)
        # matches = find_kp_by_embedding(query_text, top_k=3)

        # if not matches:
        #     return "æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†ç‚¹ï¼ˆå‘é‡åŒ¹é…å¤±è´¥ï¼‰"

        # name, text, score = matches[0]
        # return f"{name}:\n{text}\nï¼ˆç›¸ä¼¼åº¦: {score:.3f}ï¼‰"
        return res

    return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"

# ä¸»å‡½æ•°ï¼šæ‰§è¡ŒçŸ¥è¯†å›¾è°±æŸ¥è¯¢
def query_kg(course_name, intents,query):
    results = []
    for intent in intents:
        config = INTENT_TO_KG_FIELD.get(intent)
        if not config:
            results.append((intent, "â— æœªçŸ¥æ„å›¾"))
            continue

        if config["type"] == "node":
            cypher = f'''
                MATCH (c:è¯¾ç¨‹)
                WHERE c.åç§° CONTAINS "{course_name}"
                RETURN c.{config["property"]} AS result
            '''
            value = graph.evaluate(cypher)
            results.append((intent, value if value else "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"))

        elif config["type"] == "relation":
            cypher = f'''
                MATCH (c:è¯¾ç¨‹)-[:{config["rel"]}]->(t:{config["target"]})
                WHERE c.åç§° CONTAINS "{course_name}"
                RETURN collect(t.åç§°) AS result
            '''
            value = graph.evaluate(cypher)
            if value:
                results.append((intent, "ã€".join(value)))
            else:
                fallback = multi_hop_query_kg(course_name, intent,query)
                results.append((intent, fallback))

        elif config["type"] == "multi-hop":
            value = multi_hop_query_kg(course_name, intent,query)
            results.append((intent, value))

    return results
import ast
if __name__ == '__main__':
    # openai_call("æ‚¨å¥½")
    
    while True:
        # pass
        query = input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ")
        # result = openai_call(query)
        entity=extract_entity_via_llm(query)
        result=api_call(query)
        result=ast.literal_eval(result)
        print("å®ä½“è¯†åˆ«ç»“æœ",entity)
        print(f'æ„å›¾è¯†åˆ«ç»“æœ:{result}')
        # print(query_specific_unit_kg(entity, query))  # å¿½ç•¥æ„å›¾è¯†åˆ«ï¼Œç›´æ¥æµ‹è¯•ç« èŠ‚å‡½æ•°

    # course = "å¤§æ•°æ®è®¡ç®—" 
    # intents = ["æŸ¥è¯¢è¯¾ç¨‹æè¿°", "æŸ¥è¯¢è¯¾ç¨‹æ•™æ", "æŸ¥è¯¢è¯¾ç¨‹è€ƒæ ¸æ–¹å¼"]
        for intent, item in query_kg(entity, result,query):
            print(f"{intent} â†’ {item}")






# # ğŸš€ ä¸»ç¨‹åºï¼ˆå¿…é¡»æ”¾åœ¨å‡½æ•°å¤–éƒ¨ï¼‰
# if __name__ == '__main__':
#     import ast
#     while True:
#         query = input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ")
#         entity = extract_entity_via_llm(query)
#         print("å®ä½“è¯†åˆ«ç»“æœ", entity)
#         print("ç« èŠ‚æŸ¥è¯¢æµ‹è¯•ä¸­...")
#         print(query_specific_unit_kg(entity, query))  # å¼ºåˆ¶æµ‹è¯•ç« èŠ‚æŸ¥è¯¢
